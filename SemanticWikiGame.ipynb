{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNDXNz3jRu2IinMQdKzzchp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoiGerber/EcoSystem-simulator/blob/main/SemanticWikiGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6juYSPLqtni",
        "outputId": "2772262b-9276-4976-c804-7cdbe53dc8ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "\n",
        "# Print debugging information\n",
        "print(\"Python executable:\", sys.executable)\n",
        "print(\"Python path:\", sys.path)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "# Load pre-trained model for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y11pUiSHzyqy",
        "outputId": "d8281a8c-532c-4ad0-d9ea-5f82ee42b242"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python executable: /usr/bin/python3\n",
            "Python path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Torch version: 2.3.0+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_cache = {}\n",
        "embedding_cache = {}\n",
        "distances ={}\n",
        "\n",
        "class WikiPage:\n",
        "    def __init__(self, title: str, url: str):\n",
        "        self.title = title\n",
        "        self.url = url\n",
        "        if url in page_cache:\n",
        "            self.text, self.links = page_cache[url]\n",
        "        else:\n",
        "            self.text = self.get_text()\n",
        "            self.links = self.get_links(100)  # Initially fetch 100 links\n",
        "            page_cache[url] = (self.text, self.links)\n",
        "\n",
        "    def get_text(self) -> str:\n",
        "        response = requests.get(self.url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join([para.get_text() for para in paragraphs[:10]])\n",
        "        return ' '.join(text.split()[:30])  # Get first 30 words\n",
        "\n",
        "    def get_links(self, max_links: int) -> List[str]:\n",
        "        soup = BeautifulSoup(requests.get(self.url).content, 'html.parser')\n",
        "        links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if href.startswith('/wiki/') and ':' not in href and href != '/wiki/Doi_(identifier)' and 'identifier' not in href:\n",
        "                full_url = 'https://en.wikipedia.org' + href\n",
        "                links.append(full_url)\n",
        "        random.shuffle(links)\n",
        "        return links[:max_links]\n",
        "\n",
        "def get_embedding(text: str):\n",
        "    if text in embedding_cache:\n",
        "        return embedding_cache[text]\n",
        "    embedding = model.encode(text, convert_to_tensor=True)\n",
        "    embedding_cache[text] = embedding\n",
        "    return embedding\n",
        "\n",
        "def distance_to_max_links(distance: float) -> int:\n",
        "\n",
        "\n",
        "    if 0.75 <= distance:\n",
        "        return 20\n",
        "    elif 0.65<= distance <= 0.75:\n",
        "        return 25\n",
        "    elif 0.55<= distance <= 0.65:\n",
        "        return 30\n",
        "    elif 0.40 <= distance <= 0.55:\n",
        "        return 50\n",
        "    else:\n",
        "        return 100\n",
        "\n",
        "    # Linear interpolation\n",
        "    return int(310 * np.exp(-3 * distance) - 8)\n",
        "    # return int(10 + (100 - 10) * (1.0 - distance) / (1.0 - 0.35))\n",
        "\n",
        "\n",
        "def find_goal_wikipedia(start: WikiPage, goal_title: str) -> List[str]:\n",
        "    distances[start.url] = 1\n",
        "\n",
        "    print(\"Starting from \"+str(start.title))\n",
        "    goal_url = f\"https://en.wikipedia.org/wiki/{goal_title}\"\n",
        "    distances[goal_url] = 0\n",
        "\n",
        "    goal_page = WikiPage(goal_title, goal_url)\n",
        "    goal_embedding = get_embedding(goal_page.text)\n",
        "\n",
        "    current_page = start\n",
        "    path = [current_page.url]\n",
        "    visited = set()\n",
        "    visited.add(current_page.url)\n",
        "\n",
        "    while True:\n",
        "        # Check if any link is the goal page\n",
        "        for link in current_page.links:\n",
        "            if link == goal_url:\n",
        "                path.append(goal_url)\n",
        "                return path\n",
        "\n",
        "        # Calculate semantic distances\n",
        "        min_distance = float('inf')\n",
        "        next_page_url = None\n",
        "        for link in current_page.links:\n",
        "            if link in visited:\n",
        "                continue\n",
        "            link_page = WikiPage(link.split('/wiki/')[1], link)\n",
        "            link_embedding = get_embedding(link_page.text)\n",
        "            distance = 1 - util.cos_sim(link_embedding, goal_embedding).item()\n",
        "            if distance < min_distance and distance > 0:  # Ensure distance > 0 to avoid self-loops\n",
        "                min_distance = distance\n",
        "                next_page_url = link\n",
        "\n",
        "        if next_page_url is None or next_page_url in visited:\n",
        "            print(\"Stuck, unable to find a valid next page. Terminating search.\")\n",
        "            break\n",
        "\n",
        "        # Move to the next page with the smallest semantic distance\n",
        "        current_page = WikiPage(next_page_url.split('/wiki/')[1], next_page_url)\n",
        "        path.append(current_page.url)\n",
        "        visited.add(current_page.url)\n",
        "\n",
        "        print(f\"Moved to {current_page.title}\"+\"  |  \"+ f\"Distance: {min_distance}\"+\"  |  \"+f\"RandomLinks:{distance_to_max_links(min_distance)}\")\n",
        "        distances[current_page.url] = min_distance\n",
        "        # print(f\"Moved to {current_page.title} Distance: {min_distance} RandomLinks:{distance_to_max_links(min_distance)} \")\n",
        "\n",
        "        # Update the number of links to fetch based on the distance\n",
        "        max_links = distance_to_max_links(min_distance)\n",
        "        current_page.links = current_page.get_links(max_links)\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_title_from_url(url: str) -> str:\n",
        "    return url.split('/wiki/')[-1]\n",
        "\n",
        "# Example usage:\n",
        "start_title = \"2024_United_States_presidential_election\"\n",
        "goal_title = \"LinkedIn\"\n",
        "start_url = f\"https://en.wikipedia.org/wiki/{start_title}\"\n",
        "\n",
        "start_page = WikiPage(start_title, start_url)\n",
        "path = find_goal_wikipedia(start_page, goal_title)\n",
        "\n",
        "\n",
        "for url in path:\n",
        "  title = get_title_from_url(url)\n",
        "  dist = distances[url]\n",
        "  print(f\"vector distance = {dist:.2f}   |   \" + title)\n",
        "  print('V')\n",
        "print(\"Done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSSHWmhJxKwJ",
        "outputId": "063fd055-23d7-4289-c13a-43ba15844ea3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from 2024_United_States_presidential_election\n",
            "Moved to FiveThirtyEight  |  Distance: 0.7328414916992188  |  RandomLinks:25\n",
            "Moved to WHOIS  |  Distance: 0.7304809987545013  |  RandomLinks:25\n",
            "Moved to World_Wide_Web  |  Distance: 0.6202999353408813  |  RandomLinks:30\n",
            "Moved to HReview  |  Distance: 0.6110538244247437  |  RandomLinks:30\n",
            "Moved to Solid_(web_decentralization_project)  |  Distance: 0.4586864113807678  |  RandomLinks:50\n",
            "Moved to Distributed_social_network  |  Distance: 0.5660229921340942  |  RandomLinks:30\n",
            "Moved to Social_web  |  Distance: 0.4944614768028259  |  RandomLinks:50\n",
            "Moved to Mobile_social_network  |  Distance: 0.5505470931529999  |  RandomLinks:30\n",
            "Moved to Twitter  |  Distance: 0.5633731484413147  |  RandomLinks:30\n",
            "Moved to Yahoo!_Kickstart  |  Distance: 0.4338139295578003  |  RandomLinks:50\n",
            "vector distance = 1.00   |   2024_United_States_presidential_election\n",
            "|\n",
            "V\n",
            "vector distance = 0.73   |   FiveThirtyEight\n",
            "|\n",
            "V\n",
            "vector distance = 0.73   |   WHOIS\n",
            "|\n",
            "V\n",
            "vector distance = 0.62   |   World_Wide_Web\n",
            "|\n",
            "V\n",
            "vector distance = 0.61   |   HReview\n",
            "|\n",
            "V\n",
            "vector distance = 0.46   |   Solid_(web_decentralization_project)\n",
            "|\n",
            "V\n",
            "vector distance = 0.57   |   Distributed_social_network\n",
            "|\n",
            "V\n",
            "vector distance = 0.49   |   Social_web\n",
            "|\n",
            "V\n",
            "vector distance = 0.55   |   Mobile_social_network\n",
            "|\n",
            "V\n",
            "vector distance = 0.56   |   Twitter\n",
            "|\n",
            "V\n",
            "vector distance = 0.43   |   Yahoo!_Kickstart\n",
            "|\n",
            "V\n",
            "vector distance = 0.00   |   LinkedIn\n",
            "|\n",
            "V\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}